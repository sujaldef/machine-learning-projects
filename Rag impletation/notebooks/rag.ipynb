{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee54b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sujal\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup ---\n",
    "import fitz\n",
    "import re \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a75cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernand\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Load PDF ---\n",
    "def load_pdf(path):\n",
    "    \"\"\"Extract text from PDF.\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    return \"\\n\".join(page.get_text() for page in doc)\n",
    "\n",
    "pdf_text = load_pdf(\"../data/RAW.pdf\")\n",
    "print(pdf_text[:300])  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bbd1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernand\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Clean text ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', '\\n', text)           # collapse newlines\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)         # collapse spaces\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)  # remove page numbers\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_text = clean_text(pdf_text)\n",
    "print(cleaned_text[:300])  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf69e9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created: 376\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernand\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Chunk text ---\n",
    "def chunk_text(text, chunk_size=800, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(cleaned_text)\n",
    "print(f\"Chunks created: {len(chunks)}\")\n",
    "print(chunks[0][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbb88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 376 chunks with embeddings of dim 384\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Embed chunks ---\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "vector_store = {\n",
    "    \"chunks\": chunks,\n",
    "    \"embeddings\": embeddings\n",
    "}\n",
    "\n",
    "print(f\"Stored {len(chunks)} chunks with embeddings of dim {embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6811f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "e at https://ai.meta.\n",
      "com/resources/models-and-libraries/llama/. Those who use Llama 2 must comply with the terms of\n",
      "the provided license and our Acceptable Use Policy, which prohibit any uses that would violate applicable\n",
      "policies, laws, rules, and regulations.\n",
      "We also provide code examples to help\n",
      "---\n",
      "te has been in\n",
      "English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\n",
      "Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\n",
      "produce inaccurate or objectionable responses to user prompts. Therefore, before deployi\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Retriever ---\n",
    "def retrieve(query, top_k=3):\n",
    "    query_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    sims = np.dot(vector_store[\"embeddings\"], query_emb.T).squeeze()\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    return [vector_store[\"chunks\"][i] for i in idx]\n",
    "\n",
    "# Test retrieval\n",
    "results = retrieve(\"Who are the authors of LLaMA 2?\", top_k=2)\n",
    "for r in results:\n",
    "    print(\"---\")\n",
    "    print(r[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af48f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDUOZunebNGQMazQoQCPycKHeHhdYDQEc0\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57936b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) combines large language models with external knowledge sources.  It retrieves relevant information to augment the model's context before generating a response, improving accuracy and reducing hallucinations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Explain Retrieval-Augmented Generation in 2 lines\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced1ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text states that 2 trillion tokens of data were used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: RAG QA with Gemini ---\n",
    "def rag_answer(query):\n",
    "    context = \"\\n\\n\".join(retrieve(query, top_k=3))\n",
    "    prompt = f\"Answer the question using the context below. If context is not enough, say so.\\n\\n{context}\\n\\nQ: {query}\\nA:\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "print(rag_answer(\"how many trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sourcein an effort to increase knowledge and dampen hallucinations. \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a890fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The provided text does not name the authors of Llama 2.  It mentions Meta's release of the model but doesn't list individual contributors.\n",
      "\n",
      "==================================================\n",
      "Based on the provided text, LLaMA 2 is a large language model released openly for both research and commercial use.  The purpose is to encourage responsible AI innovation.  However, the text also emphasizes that users must comply with licensing and acceptable use policies and that safety testing and tuning are crucial before deployment to mitigate potential risks of inaccurate or objectionable outputs.\n",
      "\n",
      "==================================================\n",
      "The provided text states that Llama 2-Chat is a fine-tuned version of Llama 2, showing improvements in truthfulness and toxicity.  However, it does not describe the specific fine-tuning methods used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Test RAG ---\n",
    "questions = [\n",
    "    \"Who are the authors of LLaMA 2?\",\n",
    "    \"What is the purpose of LLaMA 2?\",\n",
    "    \"How is LLaMA 2 fine-tuned?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"=\"*50)\n",
    "    print(rag_answer(q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28ba4d",
   "metadata": {},
   "source": [
    "## ✅ RAG Pipeline Progress\n",
    "\n",
    "***1. Data ingestion:*** Loaded `RAW.pdf` (LLaMA 2 research paper).  \n",
    "\n",
    "***2. Chunking:*** Split the PDF text into smaller overlapping chunks.  \n",
    "\n",
    "***3. Embeddings:*** Encoded chunks into vectors using `sentence-transformers`.  \n",
    "\n",
    "At this stage → we already have a working in-memory vector store.  \n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Next Steps to Complete the Pipeline\n",
    "\n",
    "***4. Vector DB:*** Using a Python dict (ok for small scale).  \n",
    "For production: **FAISS, Pinecone, Weaviate, or Chroma**.  \n",
    "\n",
    "***5. Retrieval:*** Implemented with the `retrieve()` function.  \n",
    "\n",
    "***6. Reranking (optional):*** Not added yet, can improve ordering later.  \n",
    "\n",
    "***7. Prompt building:*** Implemented in **Cell 7** with a structured prompt.  \n",
    "\n",
    "***8. LLM call:*** Using **Gemini Flash API**.  \n",
    "\n",
    "***9. Output formatting:*** Returning **answer + sources** for transparency.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed92a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
