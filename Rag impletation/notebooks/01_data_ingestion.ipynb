{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69277ede",
   "metadata": {},
   "source": [
    "## Data Ingestion — Preparing Documents for RAG\n",
    "\n",
    "***Goal:***  \n",
    "Learn how to load, clean, and normalize documents from various sources (PDFs, text files, APIs, databases) for downstream chunking and embedding.\n",
    "\n",
    "---\n",
    "\n",
    "***Why this step matters:***\n",
    "- **Garbage in → Garbage out:** Quality of retrieved answers depends on quality of source text.  \n",
    "- **Format diversity:** Sources can be PDFs, HTML, JSON, CSV, DBs.  \n",
    "- **Preprocessing cost:** Better to normalize early than patch later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44223bd",
   "metadata": {},
   "source": [
    "### Common Sources for RAG Pipelines\n",
    "\n",
    "***Local Files:***\n",
    "- PDFs\n",
    "- Word docs (.docx)\n",
    "- Plain text (.txt)\n",
    "- Markdown (.md)\n",
    "\n",
    "***Web Sources:***\n",
    "- HTML pages\n",
    "- API responses (JSON/XML)\n",
    "\n",
    "***Databases:***\n",
    "- SQL (MySQL, PostgreSQL, SQLite)\n",
    "- NoSQL (MongoDB, Elasticsearch)\n",
    "\n",
    "***Other:***\n",
    "- Email archives\n",
    "- Spreadsheets (Excel, CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf34f6",
   "metadata": {},
   "source": [
    "### Common Sources for RAG Pipelines\n",
    "\n",
    "***Local Files:***\n",
    "- PDFs\n",
    "- Word docs (.docx)\n",
    "- Plain text (.txt)\n",
    "- Markdown (.md)\n",
    "\n",
    "***Web Sources:***\n",
    "- HTML pages\n",
    "- API responses (JSON/XML)\n",
    "\n",
    "***Databases:***\n",
    "- SQL (MySQL, PostgreSQL, SQLite)\n",
    "- NoSQL (MongoDB, Elasticsearch)\n",
    "\n",
    "***Other:***\n",
    "- Email archives\n",
    "- Spreadsheets (Excel, CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135f688",
   "metadata": {},
   "source": [
    "### Standard Data Ingestion Pipeline\n",
    "\n",
    "***1. Load:*** Read the raw document from source.  \n",
    "***2. Parse:*** Extract text and basic structure (headings, tables, metadata).  \n",
    "***3. Clean:*** Remove noise (extra spaces, headers/footers, HTML tags).  \n",
    "***4. Normalize:*** Convert to UTF-8, unify newline style, lowercase (if applicable).  \n",
    "***5. Store:*** Save in a standardized internal format (JSON or plain text).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f267b3d",
   "metadata": {},
   "source": [
    "### your pdf that you take in data will completely determines what your RAG can answer.\n",
    "### Think of it like this:\n",
    "\n",
    "The retrieval step can only find information that exists in your document store.\n",
    "\n",
    "The generation step (LLM) will base its answer on the retrieved text.\n",
    "\n",
    "So if your PDF is a research paper about transformers, your RAG will be great at answering:\n",
    "\n",
    "#### “What is the architecture of the proposed model?”\n",
    "but useless for:\n",
    "#### “What was Microsoft’s revenue in 2024?”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd71dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n"
     ]
    }
   ],
   "source": [
    "import fitz  \n",
    "\n",
    "def load_pdf(path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text() + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "# Load your research paper\n",
    "pdf_path = \"E:/Sujal/Machine learning projects/Rag impletation/data/RAW.pdf\"\n",
    "\n",
    "pdf_text = load_pdf(pdf_path)\n",
    "\n",
    "# Preview first 500 characters\n",
    "print(pdf_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e8c25",
   "metadata": {},
   "source": [
    "### Clean the pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea1cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2 [cs.CL] 19 Jul 2023\n",
      "Contents\n",
      "\n",
      "Introduction\n",
      "\n",
      "\n",
      "Pretraining\n",
      "\n",
      "2.1\n",
      "Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "2.2\n",
      "Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "2.3\n",
      "Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "\n",
      "Fine-tuning\n",
      "\n",
      "3.1\n",
      "Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "3.2\n",
      "Reinforcement Learning with Human Feedback (RLHF)\n",
      ". . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "3.3\n",
      "System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "3.4\n",
      "RLHF Results\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "\n",
      "Safety\n",
      "\n",
      "4.1\n",
      "Safety in Pretraining\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "4.3\n",
      "Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "4.4\n",
      "Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "\n",
      "Discussion\n",
      "\n",
      "5.1\n",
      "Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "5.2\n",
      "Limitations and Ethical Considerations\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "5.3\n",
      "Responsible Release Strategy\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "\n",
      "Related Work\n",
      "\n",
      "\n",
      "Conclusion\n",
      "\n",
      "A Appendix\n",
      "\n",
      "A.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "A.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "A.3 Additional Details for Fine-tuning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "A.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "A.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "A.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "A.7 Model Card\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "\n",
      "Figure 1: Helpfulness human evaluation results for Llama\n",
      "2-Chat compared to other open-source and closed-source\n",
      "models. Human raters compared model generations on ~4k\n",
      "prompts consisting of both single and multi-turn prompts.\n",
      "The 95% confidence intervals for this evaluation are between\n",
      "1% and 2%. More details in Section 3.4.2. While reviewing\n",
      "these results, it is important to note that human evaluations\n",
      "can be noisy due to limitations of the prompt set, subjectivity\n",
      "of the review guidelines, subjectivity of individual raters,\n",
      "and the inherent difficulty of comparing generations.\n",
      "Figure 2: Win-rate % for helpfulness and\n",
      "safety between commercial-licensed base-\n",
      "lines and Llama 2-Chat, according to GPT-\n",
      "4. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text.\"\"\"\n",
    "    text = re.sub(r'\\n+', '\\n', text)           # collapse multiple newlines\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)         # collapse spaces/tabs\n",
    "    text = re.sub(r'\\s+\\n', '\\n', text)         # remove spaces before newlines\n",
    "     # Remove page numbers (standalone digits on a line)\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Clean your extracted text\n",
    "cleaned_text = clean_text(pdf_text)\n",
    "\n",
    "# Preview first 500 characters\n",
    "print(cleaned_text[:5000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041080c",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "#### Why We Chunk\n",
    "\n",
    "- **Smaller chunks = better retrieval** in RAG.  \n",
    "- **Overlap preserves context** across chunks.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c4e605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 376\n",
      "--- Preview first chunk ---\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqi\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Chunk text ---\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into chunks for retrieval.\n",
    "    \n",
    "    chunk_size: number of characters per chunk\n",
    "    overlap: number of characters to overlap between chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Create chunks from cleaned text\n",
    "chunks = chunk_text(cleaned_text, chunk_size=800, overlap=100)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(\"--- Preview first chunk ---\")\n",
    "print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd18c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Cell 7: Embed and store chunks ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load an embedding model (small & fast for learning)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Embed and store chunks ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load an embedding model (small & fast for learning)\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# In-memory \"vector store\"\n",
    "vector_store = {\n",
    "    \"chunks\": chunks,\n",
    "    \"embeddings\": embeddings\n",
    "}\n",
    "\n",
    "print(f\"Stored {len(chunks)} chunks with embeddings.\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a6bdf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m v / norms\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# normalize embeddings once for cosine similarity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m embeddings_norm = normalize(\u001b[43mvector_store\u001b[49m[\u001b[33m'\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mfloat\u001b[39m))\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(query, top_k=\u001b[32m5\u001b[39m):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    Return top_k chunks most similar to query.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Uses sentence-transformers model to embed the query.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Simple retrieval (cosine similarity) ---\n",
    "import numpy as np\n",
    "\n",
    "def normalize(v):\n",
    "    \"\"\"L2-normalize a 2D numpy array of vectors (inplace copy).\"\"\"\n",
    "    norms = np.linalg.norm(v, axis=1, keepdims=True)\n",
    "    # avoid division by zero\n",
    "    norms[norms == 0] = 1.0\n",
    "    return v / norms\n",
    "\n",
    "# normalize embeddings once for cosine similarity\n",
    "embeddings_norm = normalize(vector_store['embeddings'].astype(float))\n",
    "\n",
    "def retrieve(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Return top_k chunks most similar to query.\n",
    "    Uses sentence-transformers model to embed the query.\n",
    "    \"\"\"\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True).astype(float)\n",
    "    q_emb = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)\n",
    "    sims = np.dot(embeddings_norm, q_emb.T).squeeze()  # cosine similarities\n",
    "    idx = np.argsort(-sims)[:top_k]  # top-k descending\n",
    "    results = [{\"score\": float(sims[i]), \"chunk\": vector_store['chunks'][i], \"index\": int(i)} for i in idx]\n",
    "    return results\n",
    "\n",
    "# Demo retrieval\n",
    "q = \"Who are the authors and contributors of LLaMA 2?\"\n",
    "res = retrieve(q, top_k=3)\n",
    "for i, r in enumerate(res, 1):\n",
    "    print(f\"Result {i} — score: {r['score']:.4f}\\n{r['chunk'][:400]}\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e827ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
